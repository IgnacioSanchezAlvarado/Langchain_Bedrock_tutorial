{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8090069-4f2a-4062-841a-df81ba8edb78",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Install Bedrock and Langchain</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9acf8-bc12-4f4e-99af-4c0481bb5d67",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d60be-018d-4f24-8311-9251f00867ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Language Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc503226-825d-432b-a9db-8be00ff1e86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0855a-7026-4e76-a24e-2554cf46decb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = textgen_llm(\"\"\"\n",
    "SystemMessage='you are a nice AI bot that helps users with recommendations',\n",
    "HumanMessage='give me a list of cities to visit in Germany, add a brief description of each',\n",
    "\"\"\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbbb17-67bb-4535-8886-609f8caa3ba7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Langchain Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f23b2b-6c3e-4d65-b7e9-bf9ca64389b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Prompts</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2943db3-d863-4305-8ada-ce39b523c116",
   "metadata": {},
   "source": [
    "**Simple prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3e742-9d2f-463e-919d-0fbdbbc68429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with this sentence?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87e2dd-28c5-4426-880c-b1f124f9a0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "textgen_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895d139-04c0-4a2b-be44-11d6bf90a209",
   "metadata": {},
   "source": [
    "**Prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0c2f5-f134-455c-8c6a-9d4c0917344e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80257ba6-24aa-4a25-880d-e6f41a615f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "I want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8771a-2c6d-4aa2-8e48-a7d61ea2800e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=\"Munich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443ec6e-00ec-4723-8226-105d78fb768b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Final prompt:{final_prompt}\")\n",
    "print(\"-------------\")\n",
    "\n",
    "print(f\"LLM response: {textgen_llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72f300-46c2-4873-b439-21458a1ab507",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Output Parsers</h3>\n",
    "\n",
    "To format the output of a model.\n",
    "1. Format instructions\n",
    "2. Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff2027-a48b-4e51-9960-757643e4addf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ca4fc-9378-448c-afb5-cb8285af28c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"user_input\", description=\"This the users input\"),\n",
    "    ResponseSchema(name=\"emoji\", description=\"This is your response, a reformatted response only with the emoji\"),\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9be05-3c14-45df-a928-3c0fbadd3c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453477da-dc56-455e-928f-37de757d09a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will be given a user input.\n",
    "extract the emoji that is inside it and answer back only with the emoji.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"I am playing football\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30828e30-a9ae-498c-a266-72f5e03bb01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = textgen_llm(promptValue)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad274621-de28-4fa0-8de4-b92e8fbc049f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "textgen_llm(promptValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babab6ed-9702-4ac4-9ee5-87b15fd52eb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Document loaders\n",
    "\n",
    "Langchain document loaders: https://python.langchain.com/docs/modules/data_connection/document_loaders/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841ce3f-2c16-418c-a6a4-d9a782ed1bcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "**PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbcbaa3-88c7-4f2a-959e-9146dea07bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b217f9a-23d3-433d-9235-200dfa0bac3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"AMZN-2022-Shareholder-Letter.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39c0ed-8438-4504-be87-4b93bea1ad77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ede814-b157-466e-8094-1113f5ceaa50",
   "metadata": {},
   "source": [
    "**Web scrapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0079582-d512-479a-b943-b614dc1a70a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35b3cb-90b4-4c8e-b333-5c9aff061475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = ['https://www.espn.com/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1f1b4-ede9-451e-aa98-3d693863a37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef329766-eaad-4644-82b2-3e2cf7adefa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ffc4e-b82e-4931-91e8-2944227da981",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3> Chains </h3> \n",
    "https://python.langchain.com/docs/modules/chains/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355b543-5fc7-41b5-aa84-b3db4f81992f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Simple sequential chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b008a-87ea-4c0d-a4b2-2f440986ef5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fefddd-83a4-4f98-8b69-20dfce95ba8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "your job is to come up with a classic dish from the area that the users suggests. The name of the dish has to be in English.\n",
    "% USER LOCATION:\n",
    "{user_location}\n",
    "\n",
    "%YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['user_location'])\n",
    "\n",
    "location_chain = LLMChain(llm=textgen_llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c8e86-a3ca-40a4-a3e6-04017253db47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL:\n",
    "{user_meal}\n",
    "\n",
    "% YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "promt_template = PromptTemplate(template=template, input_variables=['user_meal'])\n",
    "\n",
    "meal_chain = LLMChain(llm=textgen_llm, prompt=promt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384e25f-9bb2-44bc-8ca9-5cef25d2b05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overal_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27caf0ee-12ff-41f0-a55f-721fe08160c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = overal_chain.run(\"Paris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42999eaf-ad91-4fb4-90ba-2b1a6db2cac8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Langchain RAG: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221433b6-4a9a-44be-a297-597d6e28fd15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f2aac-087f-482c-9ba7-ae511366bcd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = ['https://www.barcelonaturisme.com/wv3/en/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4d0a2-8dc1-4b3a-a2fc-426d0f0a4b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9af1b-4ff2-4825-b27e-0957815da556",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bfc95-c281-4dcb-9777-3f57fbdfb14a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator='\\n', \n",
    "                                      chunk_size=2000, \n",
    "                                      chunk_overlap=200)\n",
    "\n",
    "\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80abef3b-830b-41d6-9748-2202b88c4ec2",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c85f7-6bd8-4a9b-afa1-dcaf4d22588e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\",\n",
    "    client=boto3_bedrock\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b43929-04bd-4b33-a4e1-56f247943894",
   "metadata": {},
   "source": [
    "### Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e21e8e-5844-4c40-a93c-5c65c83fca29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "embeddings = bedrock_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e222ce-bc28-42fc-b195-17b1328c8cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(data, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dda2d1-d47d-41ab-a5c7-d661f58004e1",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50f078-d948-4d32-b64a-49223a208447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer from the context, just say you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=textgen_llm,chain_type=\"stuff\",retriever=docsearch.as_retriever(),return_source_documents=True,chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c959-9a6f-45b1-8268-e3eebef3c8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = qa({\"query\": \"What can I do in Barcelona?\"})\n",
    "print_ww(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ecc40-e013-4d03-8d61-6fbb3ac7daca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = qa({\"query\": \"What are the monthly events in Rome this month?\"})\n",
    "print_ww(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92521f3e-40cf-4df0-8b41-834072003cd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Langchain advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16b674-08f9-46ba-a4a8-82df4eb354ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3> Chat History Basic</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b7fdf-6b61-4e4b-bef0-14b336661376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb352a-99df-4294-8408-cbb2aa192fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=textgen_llm, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525061d-2179-4bfc-8dd0-c10573e67244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_ww(conversation.predict(input=\"which countries can you recommend to visit?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b54a93-f37c-400f-892a-f1dab8a6d700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c685719-06a7-411c-b6dc-1e2ddb668e90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Chat history dynamodb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10fae4-9f7c-4b3b-b22c-2c56a7a6a940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "\n",
    "# Create the DynamoDB table.\n",
    "table = dynamodb.create_table(\n",
    "    TableName=\"SessionTable\",\n",
    "    KeySchema=[{\"AttributeName\": \"SessionId\", \"KeyType\": \"HASH\"}],\n",
    "    AttributeDefinitions=[{\"AttributeName\": \"SessionId\", \"AttributeType\": \"S\"}],\n",
    "    BillingMode=\"PAY_PER_REQUEST\",\n",
    ")\n",
    "\n",
    "# Wait until the table exists.\n",
    "table.meta.client.get_waiter(\"table_exists\").wait(TableName=\"SessionTable\")\n",
    "\n",
    "# Print out some data about the table.\n",
    "print(table.item_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b96bae-23c6-4bca-9a1b-513ac6de950b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory.chat_message_histories import DynamoDBChatMessageHistory\n",
    "\n",
    "memory_db1 = DynamoDBChatMessageHistory(table_name=\"SessionTable\", session_id=\"4\")\n",
    "\n",
    "memory_db2 = DynamoDBChatMessageHistory(table_name=\"SessionTable\", session_id=\"5\")\n",
    "\n",
    "memory1 = ConversationBufferMemory(\n",
    "    memory_key=\"history\", chat_memory=memory_db1, return_messages=True\n",
    ")\n",
    "\n",
    "memory2 = ConversationBufferMemory(\n",
    "    memory_key=\"history\", chat_memory=memory_db2, return_messages=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2ae75-6705-4bbe-bc58-cb6d6c6235b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=textgen_llm, verbose=True, memory=memory2\n",
    ")\n",
    "\n",
    "print_ww(conversation.predict(input=\"Hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472e8ea-ee07-4f43-b92a-4d5ee0d068d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3> Agents </h3>\n",
    "\n",
    "You use the LLM not just for text output but for decision making. 1. Agent, 2. Tool (like plugins), 3.Toolkit (colletion of plugins)\n",
    "\n",
    "list of tools: https://python.langchain.com/docs/integrations/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e56be-f7dc-4c86-9c47-9cd1ce1fdf2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = \"71c02d43a64cae1b3a07a3dc16b6abb3dcd9fe98f55a99680419509a90255552\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d4f46-5e46-4a8a-8e42-a9132bebacde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b9162-ea56-4cac-9b03-aeee4934d1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "react_agent_llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", model_kwargs=model_parameter)\n",
    "math_chain_llm = Bedrock(model_id=\"anthropic.claude-instant-v1\",\n",
    "                         model_kwargs={\"temperature\":0,\"stop_sequences\" : [\"```output\"]})\n",
    "\n",
    "tools = load_tools([\"serpapi\"], llm=react_agent_llm)\n",
    "\n",
    "llm_math_chain = LLMMathChain(llm=math_chain_llm, verbose=True)\n",
    "\n",
    "llm_math_chain.llm_chain.prompt.template = \"\"\"Human: Given a question with a math problem, provide only a single line mathematical expression that solves the problem in the following format. Don't solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Assistant:\n",
    " Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "tools.append(\n",
    "    Tool.from_function(\n",
    "        func=llm_math_chain.run,\n",
    "        name=\"Calculator\",\n",
    "        description=\"Useful for when you need to answer questions about math.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb4285-a80c-4857-8483-f3c9cd62f038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_agent = initialize_agent(tools, \n",
    "                               react_agent_llm, \n",
    "                               agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                               verbose=True,\n",
    "                            #    max_iteration=2,\n",
    "                            #    return_intermediate_steps=True,\n",
    "                            #    handle_parsing_errors=True,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808b112-673e-4c35-9029-1806db243019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [\"Search\", \"Calculator\"]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131d1a5-f311-48fd-89f4-d6dfe24033d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_agent.agent.llm_chain.prompt.template=prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973dd7d-7eb9-41b7-9c95-7611d22a2168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What is Amazon Sagemaker? What is the launch year multiplied by 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610ff6f-1530-47e8-a534-cc5e436e3a8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_agent(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6584d-6ac5-4bb9-9e1f-8c0037eafb99",
   "metadata": {},
   "source": [
    "### Query agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab7e3e-caef-42b1-ac94-c74fae8350ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock, model_kwargs=model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd61f46-f9ec-470f-91c5-518ce1923bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_table=[\n",
    "  {\n",
    "    \"id\": 1, \n",
    "    \"first_name\": \"John\", \n",
    "    \"last_name\": \"Doe\",\n",
    "    \"age\": 35,\n",
    "    \"postal_code\": \"90210\"\n",
    "  },\n",
    "  {  \n",
    "    \"id\": 2,\n",
    "    \"first_name\": \"Jane\",\n",
    "    \"last_name\": \"Smith\", \n",
    "    \"age\": 27,\n",
    "    \"postal_code\": \"12345\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3, \n",
    "    \"first_name\": \"Bob\",\n",
    "    \"last_name\": \"Jones\",\n",
    "    \"age\": 42,\n",
    "    \"postal_code\": \"55555\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"first_name\": \"Sara\", \n",
    "    \"last_name\": \"Miller\",\n",
    "    \"age\": 29, \n",
    "    \"postal_code\": \"13579\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"first_name\": \"Mark\",\n",
    "    \"last_name\": \"Davis\",\n",
    "    \"age\": 31,\n",
    "    \"postal_code\": \"02468\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"first_name\": \"Laura\",\n",
    "    \"last_name\": \"Wilson\",\n",
    "    \"age\": 24,\n",
    "    \"postal_code\": \"98765\" \n",
    "  },\n",
    "  {\n",
    "    \"id\": 7,\n",
    "    \"first_name\": \"Steve\",\n",
    "    \"last_name\": \"Moore\",\n",
    "    \"age\": 36,\n",
    "    \"postal_code\": \"11223\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 8,\n",
    "    \"first_name\": \"Michelle\",\n",
    "    \"last_name\": \"Chen\",\n",
    "    \"age\": 22,\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": 1,\n",
    "            \"description\": \"An order of 1 dozen pencils\"\n",
    "        },\n",
    "        {\n",
    "            \"order_id\": 2,\n",
    "            \"description\": \"An order of 2 markers\"\n",
    "        }\n",
    "    ],\n",
    "    \"postal_code\": \"33215\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 9,\n",
    "    \"first_name\": \"David\",\n",
    "    \"last_name\": \"Lee\",\n",
    "    \"age\": 29,\n",
    "    \"postal_code\": \"99567\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 10,\n",
    "    \"first_name\": \"Jessica\",\n",
    "    \"last_name\": \"Brown\",\n",
    "    \"age\": 18, \n",
    "    \"postal_code\": \"43210\"\n",
    "  }\n",
    "]\n",
    "\n",
    "def customer_lookup(id):\n",
    "    print(f\"search by customer {id}\")\n",
    "    for customer in customer_table:\n",
    "        if customer[\"id\"] == int(id):\n",
    "            print(f\"found customer {id} {customer}\")\n",
    "            return customer\n",
    "        \n",
    "    return None\n",
    "\n",
    "tools.append(Tool.from_function(\n",
    "        name=\"CustomerLookup\",\n",
    "        func=customer_lookup,  # Mock Function, replace with an api call\n",
    "        description=\"Use this when you need to lookup a customer by id.\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fce3b0-0785-43ea-b3e5-a7a13a6ae4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "question = \"\"\"\\n\\nHuman: write one sentence summary about the information you know about the customer with an id of 5.\n",
    "\n",
    "\\n\\nAssistant: Here is the one sentence summary: \"\"\"\n",
    "\n",
    "result = react_agent.run(question)\n",
    "\n",
    "print(f\"{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd643bc-0d82-43b7-b4c8-804dceb04509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
